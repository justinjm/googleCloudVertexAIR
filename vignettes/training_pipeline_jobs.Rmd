---
title: "Training Pipeline Jobs"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Training Pipeline Jobs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# options(googleAuthR.verbose = 0) # set when debugging
```

# Setup

```{r auth, warning=FALSE}
library(googleAuthR)
library(googleCloudVertexAIR)

options(googleAuthR.scopes.selected = "https://www.googleapis.com/auth/cloud-platform")

gar_auth_service(json_file = Sys.getenv("GAR_SERVICE_JSON"))
```

# Set global arguements

```{r gcva-global-arguements}
projectId <- Sys.getenv("GCVA_DEFAULT_PROJECT_ID")
gcva_region_set("us-central1")
gcva_project_set(projectId)
```

# Get training pipeline 

```{r}
training_pipeline <- gcva_trainingPipeline(
  trainingPipelineName = Sys.getenv("GCVA_TRAINING_PIPELINE")
    )
training_pipeline
```

## Check modelName exists

```{r}
# for gcva_model and gcva_batch_predict function 
training_pipeline$modelToUpload$name
```

# Get model from training pipeline 

```{r}
m <- gcva_model(modelName = training_pipeline$modelToUpload$name)
m
```

# Create custom container training job

First, 1) [create](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline) a `TrainingPipeline` that runs a `CustomJob` and then 2) import the resulting artifacts as a `Model`


```{r}
job <- gcva_custom_container_training_job(
  displayName = "vertex-r",
  containerUri = "us-central1-docker.pkg.dev/gc-vertex-ai-r/my-docker-repo/vertex-r:latest",
  command = c("Rscript", "train.R"),
  modelServingContainerCommand = c("Rscript", "serve.R"),
  modelServingContainerImageUri = image_uri,
  machineType = "machine-here",
  acceleratorType = "acceletor-value-here"
)
```

## print API request body [dev]

```{r}
library(jsonlite)
toJSON(job, pretty = TRUE, auto_unbox = TRUE)
```

