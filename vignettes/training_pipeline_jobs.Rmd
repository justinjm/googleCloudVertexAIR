---
title: "Training Pipeline Jobs"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Training Pipeline Jobs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# options(googleAuthR.verbose = 0) # set when debugging
```

# Setup

```{r auth, warning=FALSE}
library(googleAuthR)
library(googleCloudVertexAIR)

options(googleAuthR.scopes.selected = "https://www.googleapis.com/auth/cloud-platform")

gar_auth_service(json_file = Sys.getenv("GAR_SERVICE_JSON"))
```

# Set global arguements

```{r gcva-global-arguements}
projectId <- Sys.getenv("GCVA_DEFAULT_PROJECT_ID")
gcva_region_set("us-central1")
gcva_project_set(projectId)
```
# Get training pipeline 

```{r}
training_pipeline <- gcva_trainingPipeline(
  trainingPipelineName = "projects/442003009360/locations/us-central1/trainingPipelines/2601571779788931072")
training_pipeline
```

## Check modelName exists

```{r}
# for gcva_model and gcva_batch_predict function 
training_pipeline$modelToUpload$name
```

# Get model from training pipeline 


```{r}
m <- gcva_model(modelName = training_pipeline$modelToUpload$name)
m
```
# Batch prediction [DEV ONLY] [TODO]

You can make a batch prediction by invoking the batch_predict() method, with the following parameters:

- `job_display_name`: The human readable name for the batch prediction job.
- `gcs_source`: A list of one or more batch request input files.
- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.
- `instances_format`: The format for the input instances, either 'bigquery', 'csv' or 'jsonl'. Defaults to 'jsonl'.
- `predictions_format`: The format for the output predictions, either 'csv' or 'jsonl'. Defaults to 'jsonl'.
- `machine_type`: The type of machine to use for training.
- `accelerator_type`: The hardware accelerator type.
- `accelerator_count`: The number of accelerators to attach to a worker replica.
- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete.

```r
# Note: The bigquery_source and bigquery_destination_prefix must be in the same region
PREDICTION_RESULTS_DATASET_ID = f"{PROJECT_ID}.{RESULTS_DATASET_ID}"

batch_predict_job = model.batch_predict(
    job_display_name="tabular_regression_batch_predict_job",
    bigquery_source=f"bq://{PREDICTION_INPUT_TABLE_ID}",
    instances_format="bigquery",
    predictions_format="bigquery",
    bigquery_destination_prefix=f"bq://{PREDICTION_RESULTS_DATASET_ID}",
)
```
