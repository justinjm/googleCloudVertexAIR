---
title: "Training a tabuluar AutoML model and performing batch prediction"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{automltabularbatch}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

Emulation of notebook tutorials using Vertex AI SDK via Python: 

* [vertex-ai-samples/automl-tabular-classification.ipynb at master \| GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/automl/automl-tabular-classification.ipynb)
* [vertex-ai-mlops/02b - Vertex AI - AutoML with clients (code).ipynb at main Â· statmike/vertex-ai-mlops](https://github.com/statmike/vertex-ai-mlops/blob/main/02%20-%20Vertex%20AI%20AutoML/02b%20-%20Vertex%20AI%20-%20AutoML%20with%20clients%20(code).ipynb)

# Setup

## Installation

Run the following chunk to install `googleCloudVertexAIR` and the other required R packages to complete this tutorial (checking to see if they are installed first and only install if not already): 

```{r install_packages, eval=FALSE}
required_packages <- c("remotes", "googleAuthR")
missing_packages <- required_packages[!(required_packages %in% 
                                          installed.packages()[,"Package"])]
if(length(missing_packages)) install.packages(missing_packages)

# remotes::install_github("justinjm/googleCloudVertexAIR") # run first time 
```

```{r include=FALSE, echo=FALSE, results='hide'}
# options(googleAuthR.verbose = 0) # set when debugging  
``` 

### .Renviron

Create a file called `.Renviron` in your project's working directory and use the following environemtn arguments: 

* `GAR_SERVICE_JSON` - path to service account (JSON) keyfile downloaded before and copied 
* `GCVA_DEFAULT_PROJECT_ID` - string of your GCP project you configured before 
* `GCVA_DEFAULT_REGION` - region of GCP resorces that can be one of: `"us-central1"` or `"eu"` 

e.g. your `.Renviron` should look like: 

```
# .Renviron
GAR_SERVICE_JSON="/Users/me/auth/auth.json"
GCAT_DEFAULT_PROJECT_ID="my-project"
GCAT_DEFAULT_REGION="us-central1"
```

## Setup Google Cloud Project

## Authenticate your Google Cloud Account

```{r auth, warning=FALSE}
library(googleAuthR)
library(googleCloudVertexAIR)

options(googleAuthR.scopes.selected = "https://www.googleapis.com/auth/cloud-platform")

gar_auth_service(json_file = Sys.getenv("GAR_SERVICE_JSON"))
```

## Set global arguements

```{r gcva-global-arguements}
projectId <- Sys.getenv("GCVA_DEFAULT_PROJECT_ID")
gcva_region_set(region = "us-central1")
gcva_project_set(projectId = projectId)

timestamp <- strftime(Sys.time(), "%Y%m%d%H%M%S")
timestamp
```

## Set Vertex AI managed dataset display name
```{r set-datsetDisplayName}
datasetDisplayName <- sprintf("california-housing-%s", timestamp)
datasetDisplayName
```

## Create a Cloud Storage bucket

TODO 

# Tutorial

## Create Dataset

Source dataset: `gs://cloud-samples-data/ai-platform-unified/datasets/tabular/california-housing-tabular-regression.csv`

```{r create-dataset}
dataset <- gcva_create_tabluar_dataset(
  displayName = datasetDisplayName,
  gcsSource = "gs://cloud-samples-data/ai-platform-unified/datasets/tabular/california-housing-tabular-regression.csv")
dataset
```

## Train Model

### Create training pipeline

* Doc: https://cloud.google.com/vertex-ai/docs/training/automl-api#aiplatform_create_training_pipeline_tabular_classification_sample-drest
* API: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines)

```{r create-automl-tabular-training-job}
job <- gcva_automl_tabluar_training_job(
  displayName = sprintf("california-housing-%s", timestamp),
  optimizationPredictionType = "regression",
  column_transformations = list(
          list(numeric     = list(column_name = "longitude")),
          list(numeric     = list(column_name = "latitude")),
          list(numeric     = list(column_name = "housing_median_age")),
          list(numeric     = list(column_name = "total_rooms")),
          list(numeric     = list(column_name = "total_bedrooms")),
          list(numeric     = list(column_name = "population")),
          list(numeric     = list(column_name = "households")),
          list(numeric     = list(column_name = "median_income"))
          )
  )
```


### Run the training pipeline

```r
trainingPipelineJob <- gcva_run_job(
  job = job,
  dataset = dataset,
  targetColumn = "median_house_value",
  modelDisplayName = sprintf("model-%s", datasetDisplayName))

model <- trainingPipelineJob$modelToUpload$name
model 
```

```{r eval=FALSE, include=FALSE, echo=FALSE, results='hide'}
saveRDS(trainingPipelineJob, 
        file = here::here(
          "vignettes",sprintf("trainingPipelineJob-response-%s.rds",
                              datasetDisplayName)
        )
)
```

## Make a batch prediction request

* Doc: https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#aiplatform_create_batch_prediction_job_bigquery_sample-python
* API: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs

### Make test items

TODO 

### Make batch input file

TODO 

### Make the batch prediction request

#### set constants

```{r set-constants-for-batch-prediction}
(bq_source_uri <- sprintf("bq://%s.california_housing.batch_01", projectId))
(bq_destination_prefix <- sprintf("bq://%s.california_housing", projectId))
```

#### execute request

https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs/create


```{r batch-prediction-job}
# hard code #modelName for testing purposes, model state = completed 
model <- "projects/442003009360/locations/us-central1/models/2734102788232445952"

batch_prediction_job <- gcva_batch_predict(
  jobDisplayName = sprintf("california-housing-%s", timestamp),
  model = model, #modelName
  bigquerySource= bq_source_uri,
  instancesFormat = "bigquery",
  predictionsFormat = "bigquery",
  bigqueryDestinationPrefix = bq_destination_prefix
)
batch_prediction_job
```

### Wait for completion of batch prediction job

Once the batch prediction job has completed, you can then view and use the predictions 

### Get predictions

TODO 

## Cleaning up

TODO

```r
gcva_delete_job()
gcva_delete_model()
```

```{r cleanup-delete-dataset}
gcva_delete_dataset(displayName = dataset$displayName)
```

## Reference

* Create training pipeline
  * [Vertex AI](https://cloud.google.com/vertex-ai/docs/training/automl-api#aiplatform_create_training_pipeline_tabular_classification_sample-drest)
  * [Python SDK - Google Cloud](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.AutoMLTabularTrainingJob)
  * [Python SDK - GitHub](https://googleapis.dev/python/aiplatform/latest/aiplatform/services.html#google.cloud.aiplatform.AutoMLTabularTrainingJob)
  * [REST API](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines)
* Batch Prediction 
  * https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#aiplatform_create_batch_prediction_job_bigquery_sample-python
  * https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs
  * https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/sdk_automl_tabular_regression_batch_bq.ipynb
