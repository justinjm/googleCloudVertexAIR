---
title: "Training a tabuluar AutoML model and performing batch prediction"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{automltabularbatch}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

Emulation of notebook tutorials using Vertex AI SDK via Python: 

* [vertex-ai-samples/automl-tabular-classification.ipynb at master \| GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/automl/automl-tabular-classification.ipynb)
* [vertex-ai-mlops/02b - Vertex AI - AutoML with clients (code).ipynb at main Â· statmike/vertex-ai-mlops](https://github.com/statmike/vertex-ai-mlops/blob/main/02%20-%20Vertex%20AI%20AutoML/02b%20-%20Vertex%20AI%20-%20AutoML%20with%20clients%20(code).ipynb)

# Setup

## Installation

Run the following chunk to install `googleCloudVertexAIR` and the other required R packages to complete this tutorial (checking to see if they are installed first and only install if not already): 

```{r install_packages, eval=FALSE}
required_packages <- c("remotes", "googleAuthR")
missing_packages <- required_packages[!(required_packages %in% 
                                          installed.packages()[,"Package"])]
if(length(missing_packages)) install.packages(missing_packages)
# remotes::install_github("justinjm/googleCloudVertexAIR") # run first time 
# options(googleAuthR.verbose = 0) # set when debugging  
``` 

### .Renviron

Create a file called `.Renviron` in your project's working directory and use the following environemtn arguments: 

* `GAR_SERVICE_JSON` - path to service account (JSON) keyfile downloaded before and copied 
* `GCVA_DEFAULT_PROJECT_ID` - string of your GCP project you configured before 
* `GCVA_DEFAULT_REGION` - region of GCP resorces that can be one of: `"us-central1"` or `"eu"` 

e.g. your `.Renviron` should look like: 

```
# .Renviron
GAR_SERVICE_JSON="/Users/me/auth/auth.json"
GCAT_DEFAULT_PROJECT_ID="my-project"
GCAT_DEFAULT_REGION="us-central1"
```

## Setup Google Cloud Project

## Authenticate your Google Cloud Account

```{r auth, warning=FALSE}
library(googleAuthR)
library(googleCloudVertexAIR)

options(googleAuthR.scopes.selected = "https://www.googleapis.com/auth/cloud-platform")

gar_auth_service(json_file = Sys.getenv("GAR_SERVICE_JSON"))
```

## Set global arguements

```{r gcva-global-arguements}
projectId <- Sys.getenv("GCVA_DEFAULT_PROJECT_ID")
gcva_region_set(region = "us-central1")
gcva_project_set(projectId = projectId)

timestamp <- strftime(Sys.time(), "%Y%m%d%H%M%S")
timestamp
```

## Set Vertex AI managed dataset display name 
```{r}
datasetDisplayName <- sprintf("california-housing-%s", timestamp)
datasetDisplayName
```

## Create a Cloud Storage bucket

# Tutorial

## Create Dataset

Source dataset: `gs://cloud-samples-data/ai-platform-unified/datasets/tabular/california-housing-tabular-regression.csv`

```{r}
dataset <- gcva_create_tabluar_dataset(
  displayName = datasetDisplayName,
  gcsSource = "gs://cloud-samples-data/ai-platform-unified/datasets/tabular/california-housing-tabular-regression.csv")
dataset
```

## Train Model

### Create training pipeline

docs: 

* [Vertex AI](https://cloud.google.com/vertex-ai/docs/training/automl-api#aiplatform_create_training_pipeline_tabular_classification_sample-drest)
* [Python SDK - Google Cloud](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.AutoMLTabularTrainingJob)
* [Python SDK - GitHub](https://googleapis.dev/python/aiplatform/latest/aiplatform/services.html#google.cloud.aiplatform.AutoMLTabularTrainingJob)
* [REST API](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines)

```{r}
job <- gcva_automl_tabluar_training_job(
  displayName = sprintf("california-housing-%s", timestamp),
  optimizationPredictionType = "regression",
  column_transformations = list(
          list(numeric     = list(column_name = "longitude")),
          list(numeric     = list(column_name = "latitude")),
          list(numeric     = list(column_name = "housing_median_age")),
          list(numeric     = list(column_name = "total_rooms")),
          list(numeric     = list(column_name = "total_bedrooms")),
          list(numeric     = list(column_name = "population")),
          list(numeric     = list(column_name = "households")),
          list(numeric     = list(column_name = "median_income"))
          )
  )
```


### Run the training pipeline

```r
# more R native ?
training_pipeline_job <- gcva_run_job(
  job = job,
  dataset = dataset,
  targetColumn = "median_house_value",
  modelDisplayName = sprintf("model-%s", datasetDisplayName))

modelName <- training_pipeline_job$modelToUpload$name
modelName 
```


```r
# Similar to python examples 
model <- gcva_run_job(
  job = job,
  dataset = dataset,
  targetColumn = "median_house_value",
  modelDisplayName = sprintf("model-%s", datasetDisplayName))
model
```

```r
saveRDS(model, 
        file = here::here(
          "vignettes",sprintf("model-response-%s.rds", datasetDisplayName)
          )
        )
```

```r
model_loaded <- readRDS(file = here::here(
  "vignettes", sprintf("model-response-%s.rds", datasetDisplayName)
  )
)
model_loaded
```

## Model deployment for batch prediction


## Make a batch prediction request

### Make test items

### Make batch input file

### Make the batch prediction request

https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#aiplatform_create_batch_prediction_job_bigquery_sample-python

#### set constants 


```{r}
(bq_source_uri <- paste0("bq://",projectId,".","california_housing", ".", "batch01"))
(bq_destination_prefix <- paste0("bq://",projectId,".","california_housing"))
```

#### execute request
```{r}
batch_prediction_job <- gcva_batch_predict(
  model = model, #modelName
  bigquerySource= bq_source_uri,
  instancesFormat = "bigquery",
  predictionsFormat = "bigquery",
  bigqueryDestinationPrefix = bq_destination_prefix
)
```

### Wait for completion of batch prediction job

### Get predictions

## Cleaning up

## REference 


https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/sdk_automl_tabular_regression_batch_bq.ipynb

```
You can make a batch prediction by invoking the batch_predict() method, with the following parameters:

- `job_display_name`: The human readable name for the batch prediction job.
- `gcs_source`: A list of one or more batch request input files.
- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.
- `instances_format`: The format for the input instances, either 'bigquery', 'csv' or 'jsonl'. Defaults to 'jsonl'.
- `predictions_format`: The format for the output predictions, either 'csv' or 'jsonl'. Defaults to 'jsonl'.
- `machine_type`: The type of machine to use for training.
- `accelerator_type`: The hardware accelerator type.
- `accelerator_count`: The number of accelerators to attach to a worker replica.
- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete.
```


```r
# Note: The bigquery_source and bigquery_destination_prefix must be in the same region
PREDICTION_RESULTS_DATASET_ID = f"{PROJECT_ID}.{RESULTS_DATASET_ID}"

batch_predict_job = model.batch_predict(
    job_display_name="tabular_regression_batch_predict_job",
    bigquery_source=f"bq://{PREDICTION_INPUT_TABLE_ID}",
    instances_format="bigquery",
    predictions_format="bigquery",
    bigquery_destination_prefix=f"bq://{PREDICTION_RESULTS_DATASET_ID}",
)
```
