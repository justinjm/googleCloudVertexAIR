---
title: "AutoML training tabular binary classification model for batch prediction¶"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{datasets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

Emulation of notebook tutorials using Vertex AI SDK via Python: 

* [vertex-ai-samples/automl-tabular-classification.ipynb at master \| GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/automl/automl-tabular-classification.ipynb)
* [vertex-ai-mlops/02b - Vertex AI - AutoML with clients (code).ipynb at main · statmike/vertex-ai-mlops](https://github.com/statmike/vertex-ai-mlops/blob/main/02b%20-%20Vertex%20AI%20-%20AutoML%20with%20clients%20(code).ipynb)


# Setup

## Installation

Run the following chunk to install `googleCloudVertexAIR` and the other required R packages to complete this tutorial (checking to see if they are installed first and only install if not already): 

```{r install_packages, eval=FALSE}
required_packages <- c("remotes", "googleAuthR")
missing_packages <- required_packages[!(required_packages %in% 
                                          installed.packages()[,"Package"])]
if(length(missing_packages)) install.packages(missing_packages)

# remotes::install_github("justinjm/googleCloudVertexAIR")
# options(googleAuthR.verbose = 0) # set for debugging  
``` 

### .Renviron

Create a file called `.Renviron` in your project's working directory and use the following environemtn arguments: 

* `GAR_SERVICE_JSON` - path to service account (JSON) keyfile downloaded before and copied 
* `GCVA_DEFAULT_PROJECT_ID` - string of your GCP project you configured before 
* `GCVA_DEFAULT_REGION` - region of GCP resorces that can be one of: `"us-central1"` or `"eu"` 

e.g. your `.Renviron` should look like: 

```
# .Renviron
GAR_SERVICE_JSON="/Users/me/auth/auth.json"
GCAT_DEFAULT_PROJECT_ID="my-project"
GCAT_DEFAULT_REGION="us-central1"
```

## Setup Google Cloud Project

## Authenticate your Google Cloud Account

```{r auth, warning=FALSE}
library(googleAuthR)
library(googleCloudVertexAIR)

options(googleAuthR.scopes.selected = "https://www.googleapis.com/auth/cloud-platform")

gar_auth_service(json_file = Sys.getenv("GAR_SERVICE_JSON"))
```

## Set global arguements

```{r gcva-global-arguements}
projectId <- Sys.getenv("GCVA_DEFAULT_PROJECT_ID")
gcva_region_set(region = "us-central1")
gcva_project_set(projectId = projectId)

# AutoML dataset
datasetDisplayName <- paste0("my-experiment-", strftime(Sys.time(), "%Y%m%d%H%M%S"))
datasetDisplayName
```

## Create a Cloud Storage bucket

# Tutorial

## Create Dataset

Source dataset: `gs://cloud-samples-data/ai-platform-unified/datasets/tabular/california-housing-tabular-regression.csv`



```{r}
dataset <- gcva_create_tabluar_dataset(
  displayName = datasetDisplayName,
  gcsSource = "gs://cloud-samples-data/ai-platform-unified/datasets/tabular/california-housing-tabular-regression.csv")
dataset
```

## Train Model

### Create training pipeline

docs: 

* [Vertex AI](https://cloud.google.com/vertex-ai/docs/training/automl-api#aiplatform_create_training_pipeline_tabular_classification_sample-drest)
* [Python SDK - Google Cloud](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.AutoMLTabularTrainingJob)
* [Python SDK - GitHub](https://googleapis.dev/python/aiplatform/latest/aiplatform/services.html#google.cloud.aiplatform.AutoMLTabularTrainingJob)
* [REST API](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines)

```{r}
job <- gcva_automl_tabluar_training_job(
  displayName = "test_pipeline_1",
  optimizationPredictionType = "classification",
  column_transformations = list(
          list(numeric     = list(column_name = "V1")),
          list(categorical = list(column_name = "V2")),
          list(categorical = list(column_name = "V3")),
          list(categorical = list(column_name = "V4")),
          list(categorical = list(column_name = "V5")),
          list(numeric     = list(column_name = "V6")),
          list(categorical = list(column_name = "V7")),
          list(categorical = list(column_name = "V8")),
          list(categorical = list(column_name = "V9")),
          list(numeric     = list(column_name = "V10")),
          list(categorical = list(column_name = "V11")),
          list(numeric     = list(column_name = "V12")),
          list(numeric     = list(column_name = "V13")),
          list(numeric     = list(column_name = "V14")),
          list(numeric     = list(column_name = "V15")),
          list(categorical = list(column_name = "V16"))))
job
```


### Run the training pipeline

```r 
model <- gcva_run_job(
  job = job,
  dataset = datasetDisplayName,
  targetColumn = "Class",
  modelDisplayName = "test1-classification")
model
```

```{r}
saveRDS(model, file = here::here("vignettes", "response-model2-2022-07-18.rds"))
```

```{r}
model_loaded <- readRDS(file = here::here("vignettes", "response-model2-2022-07-18.rds"))
model_loaded
```

## Model deployment for batch prediction

Now deploy the trained Vertex`Model`resource you created for batch prediction. This differs from deploying a`Model`resource for online prediction.

For online prediction, you:

1.  Create an`Endpoint`resource for deploying the`Model`resource to.
2.  Deploy the`Model`resource to the`Endpoint`resource.
3.  Make online prediction requests to the`Endpoint`resource.

For batch-prediction, you:

1.  Create a batch prediction job.
2.  The job service will provision resources for the batch prediction request.
3.  The results of the batch prediction request are returned to the caller.
4.  The job service will unprovision the resoures for the batch prediction request.

## Make a batch prediction request

### Make test items

### Make batch input file

### Make the batch prediction request
[vertex-ai-samples/sdk\_automl\_tabular\_regression\_batch\_bq.ipynb at main · GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/sdk_automl_tabular_regression_batch_bq.ipynb)

You can make a batch prediction by invoking the batch_predict() method, with the following parameters:

- `job_display_name`: The human readable name for the batch prediction job.
- `gcs_source`: A list of one or more batch request input files.
- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.
- `instances_format`: The format for the input instances, either 'bigquery', 'csv' or 'jsonl'. Defaults to 'jsonl'.
- `predictions_format`: The format for the output predictions, either 'csv' or 'jsonl'. Defaults to 'jsonl'.
- `machine_type`: The type of machine to use for training.
- `accelerator_type`: The hardware accelerator type.
- `accelerator_count`: The number of accelerators to attach to a worker replica.
- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete.

```r
# Note: The bigquery_source and bigquery_destination_prefix must be in the same region
PREDICTION_RESULTS_DATASET_ID = f"{PROJECT_ID}.{RESULTS_DATASET_ID}"

batch_predict_job = model.batch_predict(
    job_display_name="tabular_regression_batch_predict_job",
    bigquery_source=f"bq://{PREDICTION_INPUT_TABLE_ID}",
    instances_format="bigquery",
    predictions_format="bigquery",
    bigquery_destination_prefix=f"bq://{PREDICTION_RESULTS_DATASET_ID}",
)
```

### Wait for completion of batch prediction job

### Get predictions

## Cleaning up


## References

* [Vertex AI (aiplatform) AutoML components  |  Google Cloud](https://cloud.google.com/vertex-ai/docs/pipelines/vertex-automl-component) 
  - [vertex-ai-samples/automl_tabular_classification_beans.ipynb at main · GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb)
- 
