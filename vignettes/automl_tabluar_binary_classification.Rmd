---
title: "AutoML training tabular binary classification model for batch prediction¶"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{datasets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

Emulation of notebook tutorials using Vertex AI SDK via Python: 
* [vertex-ai-samples/automl-tabular-classification.ipynb at master \| GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/automl/automl-tabular-classification.ipynb)
* [vertex-ai-mlops/02b - Vertex AI - AutoML with clients (code).ipynb at main · statmike/vertex-ai-mlops](https://github.com/statmike/vertex-ai-mlops/blob/main/02b%20-%20Vertex%20AI%20-%20AutoML%20with%20clients%20(code).ipynb)

# Setup

## Installation

Run the following chunk to install `googleCloudVertexAIR` and the other required R packages to complete this tutorial (checking to see if they are installed first and only install if not already): 

```{r install_packages, eval=FALSE}
list.of.packages <- c("remotes", "googleAuthR")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

remotes::install_github("justinjm/googleCloudVertexAIR")
``` 

### .Renviron

Create a file called `.Renviron` in your project's working directory and use the following environemtn arguments: 

* `GAR_SERVICE_JSON` - path to service account (JSON) keyfile downloaded before and copied 
* `GCVA_DEFAULT_PROJECT_ID` - string of your GCP project you configured before 
* `GCVA_DEFAULT_REGION` - region of GCP resorces that can be one of: `"us-central1"` or `"eu"` 

e.g. your `.Renviron` should look like: 

```
# .Renviron
GAR_SERVICE_JSON="/Users/me/auth/auth.json"
GCAT_DEFAULT_PROJECT_ID="my-project"
GCAT_DEFAULT_REGION="us-central1"
```

## Setup Google Cloud Project

## Authenticate your Google Cloud Account

```{r auth, warning=FALSE}
library(googleAuthR)
library(googleCloudVertexAIR)

options(googleAuthR.scopes.selected = "https://www.googleapis.com/auth/cloud-platform")

gar_auth_service(json_file = Sys.getenv("GAR_SERVICE_JSON"))
```

## Set global arguements

```{r gcva-global-arguements}
projectId <- Sys.getenv("GCVA_DEFAULT_PROJECT_ID")
gcva_region_set("us-central1")
gcva_project_set(projectId)

# AutoML dataset
displayName <- paste0("my-experiment-", strftime(Sys.time(), "%Y%m%d%H%M%S"))
displayName
```

## Create a Cloud Storage bucket

# Tutorial

## Create Dataset

```{r}
dataset <- gcva_create_tabluar_dataset(displayName = displayName,
                                       gcsSource = "gs://gcatr-dev/bank_marketing.csv")
dataset
```

## Train Model

### Set columns 

Target variable: 

```{r}
# dataset <- gcva_set_target_column(columnDisplayName = "ZZZZZZZZ")
# dataset
```



https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob


#### Create training pipeline

#### Run the training pipeline

## Model deployment for batch prediction

Now deploy the trained Vertex`Model`resource you created for batch prediction. This differs from deploying a`Model`resource for online prediction.

For online prediction, you:

1.  Create an`Endpoint`resource for deploying the`Model`resource to.
2.  Deploy the`Model`resource to the`Endpoint`resource.
3.  Make online prediction requests to the`Endpoint`resource.

For batch-prediction, you:

1.  Create a batch prediction job.
2.  The job service will provision resources for the batch prediction request.
3.  The results of the batch prediction request are returned to the caller.
4.  The job service will unprovision the resoures for the batch prediction request.

## Make a batch prediction request

### Make test items

### Make batch input file

### Make the batch prediction request

### Wait for completion of batch prediction job

### Get predictions

## Cleaning up
